# ğŸ“ Folder Structure

```
your-apify-actor/
â”‚
â”œâ”€â”€ ğŸ“„ .actor/
â”‚   â””â”€â”€ input_schema.json        # Input configuration (dropdown for sources)
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ main.js                  # ğŸ¯ Main orchestrator
â”‚   â”‚                              - Routes to scrapers
â”‚   â”‚                              - Handles webhooks
â”‚   â”‚                              - Exports utilities
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ scrapers/
â”‚       â”œâ”€â”€ ontario.js           # ğŸ Ontario Tenders Portal scraper
â”‚       â”œâ”€â”€ samgov.js            # ğŸ‡ºğŸ‡¸ SAM.gov scraper
â”‚       â”œâ”€â”€ _template.js         # ğŸ“‹ Template for new scrapers
â”‚       â”‚
â”‚       â””â”€â”€ [add more here]      # ğŸš€ Your future scrapers:
â”‚           â”œâ”€â”€ ukgov.js         #    - UK Government
â”‚           â”œâ”€â”€ australia.js     #    - Australia
â”‚           â”œâ”€â”€ singapore.js     #    - Singapore
â”‚           â””â”€â”€ ...              #    - etc.
â”‚
â”œâ”€â”€ ğŸ“„ package.json              # Dependencies (apify, playwright, etc.)
â”œâ”€â”€ ğŸ“„ package-lock.json
â”œâ”€â”€ ğŸ“„ Dockerfile                # (optional) Custom build
â””â”€â”€ ğŸ“„ README.md                 # This documentation
```

## ğŸ”„ Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Apify Actor Starts                        â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                    main.js                          â”‚     â”‚
â”‚  â”‚  1. Get input: { source: "ontario", ... }         â”‚     â”‚
â”‚  â”‚  2. Launch browser                                  â”‚     â”‚
â”‚  â”‚  3. Route based on source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚
â”‚                                                       â”‚     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚               Scraper Selection                         â”‚â”‚
â”‚  â”‚                                                          â”‚â”‚
â”‚  â”‚  if source === "ontario"  â”€â”€â–¶  scrapers/ontario.js     â”‚â”‚
â”‚  â”‚  if source === "samgov"   â”€â”€â–¶  scrapers/samgov.js      â”‚â”‚
â”‚  â”‚  if source === "ukgov"    â”€â”€â–¶  scrapers/ukgov.js       â”‚â”‚
â”‚  â”‚  ...                                                     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚          Individual Scraper (e.g., ontario.js)     â”‚     â”‚
â”‚  â”‚  1. Navigate to portal                             â”‚     â”‚
â”‚  â”‚  2. Extract list items                             â”‚     â”‚
â”‚  â”‚  3. Visit each detail page                         â”‚     â”‚
â”‚  â”‚  4. Return standardized results []                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                Back to main.js                      â”‚     â”‚
â”‚  â”‚  1. Receive results from scraper                   â”‚     â”‚
â”‚  â”‚  2. Save to Apify dataset                          â”‚     â”‚
â”‚  â”‚  3. Send to webhook in batches                     â”‚     â”‚
â”‚  â”‚  4. Generate summary report                        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â–¼                                   â”‚
â”‚                    Actor Complete                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Quick Reference: Adding a New Source

### âš¡ 3-Minute Setup

1ï¸âƒ£ **Copy template:**
```bash
cp src/scrapers/_template.js src/scrapers/mynewsource.js
```

2ï¸âƒ£ **Fill in placeholders:**
```javascript
// src/scrapers/mynewsource.js
async function scrapeMyNewSource({ page, maxItems }) {
  await page.goto('https://mynewportal.com');
  // ... your logic
  return results;
}
module.exports = { scrapeMyNewSource };
```

3ï¸âƒ£ **Add to main.js router:**
```javascript
// src/main.js
case 'mynewsource':
  const { scrapeMyNewSource } = require('./scrapers/mynewsource');
  results = await scrapeMyNewSource({ page, maxItems });
  break;
```

4ï¸âƒ£ **Update input schema:**
```json
{
  "enum": ["ontario", "samgov", "mynewsource"],
  "enumTitles": ["Ontario", "SAM.gov", "My New Source"]
}
```

âœ… **Done!** Deploy and test with `source: "mynewsource"`

## ğŸ“Š Module Responsibilities

| Module | Responsibility | Touches |
|--------|---------------|---------|
| **main.js** | Routing, webhooks, orchestration | All scrapers |
| **ontario.js** | Ontario-specific scraping | Only Ontario portal |
| **samgov.js** | SAM.gov-specific scraping | Only SAM.gov portal |
| **_template.js** | Reference/starter for new scrapers | Nothing (template) |

## ğŸ”§ Shared Utilities (from main.js)

All scrapers can import:

```javascript
const { formatDateForSupabase, crypto } = require('../main');

// Use utilities
const isoDate = formatDateForSupabase('Oct 11, 2025');
const hash = crypto.createHash('sha256').update(data).digest('hex');
```

## ğŸ¨ Code Organization Benefits

### âŒ Before (Everything in one file)
```javascript
// main.js (3000+ lines) ğŸ˜±
- Ontario scraping logic (500 lines)
- SAM.gov scraping logic (600 lines)
- UK Gov scraping logic (550 lines)
- Australia scraping logic (500 lines)
- Routing logic (100 lines)
- Webhook logic (200 lines)
- Utilities (100 lines)
- ... impossible to maintain!
```

### âœ… After (Modular)
```javascript
// main.js (200 lines) âœ¨
- Routing (50 lines)
- Webhook handling (100 lines)
- Utilities (50 lines)

// scrapers/ontario.js (500 lines) ğŸ“
- Only Ontario logic

// scrapers/samgov.js (600 lines) ğŸ“
- Only SAM.gov logic

// scrapers/ukgov.js (550 lines) ğŸ“
- Only UK Gov logic
```

**Result:** Each file is focused, maintainable, and testable!

## ğŸ§ª Testing Individual Modules

```javascript
// test-ontario.js
const { chromium } = require('playwright');
const { scrapeOntario } = require('./src/scrapers/ontario');

(async () => {
  const browser = await chromium.launch({ headless: false });
  const page = await browser.newPage();
  
  const results = await scrapeOntario({
    page,
    maxItems: 2,
    webhookUrl: '',
    webhookSecret: ''
  });
  
  console.log('Results:', results.length);
  console.log('First item:', results[0]);
  
  await browser.close();
})();
```

Run: `node test-ontario.js`

## ğŸ“ Scraper Function Signature

Every scraper MUST follow this signature:

```javascript
/**
 * @param {Object} params
 * @param {Page} params.page - Playwright page object
 * @param {number} params.maxItems - Max items to scrape (0 = all)
 * @param {string} params.webhookUrl - Optional webhook URL
 * @param {string} params.webhookSecret - Optional webhook secret
 * @returns {Promise<Array>} Array of opportunity objects
 */
async function scrapeSomething({ page, maxItems, webhookUrl, webhookSecret }) {
  // Your implementation
  return results;
}
```

## ğŸ¯ Standard Return Format

Every scraper MUST return:

```javascript
[
  {
    id: "abc123...",              // SHA256 hash (40 chars)
    hash_fingerprint: "abc123...", // Same as id
    title: "Project Title",        // Required
    agency: "Organization",        // Required
    status: "Active",             // Required
    project_reference: "REF-001",  // Required
    created_at: "2025-10-11T00:00:00.000Z", // ISO 8601
    category: "Construction",
    listing_expiry_date: "2025-11-16T00:00:00.000Z", // ISO 8601
    portal_url: "https://...",    // Required
    city: "Location",
    portal_source: "Source Name",  // Required
    
    // ... additional source-specific fields
  },
  // ... more items
]
```

## ğŸš€ Deployment Checklist

Before deploying:

- [ ] All scrapers in `src/scrapers/` folder
- [ ] Each scraper exports its function
- [ ] Main.js imports and routes correctly
- [ ] Input schema updated with new sources
- [ ] Tested locally with `maxItems: 2`
- [ ] Error handling in place
- [ ] Logging statements added
- [ ] README.md updated
- [ ] No hardcoded credentials

## ğŸ’¡ Pro Tips

1. **Start with template** - Always copy `_template.js`
2. **Test small first** - Use `maxItems: 3` for initial testing
3. **Log everything** - Console.log is your friend
4. **Handle errors** - Always save basic data even if details fail
5. **Use formatDateForSupabase** - For all date fields
6. **Generate fingerprints** - For deduplication
7. **Keep scrapers independent** - No cross-imports between scrapers
8. **Document selectors** - Comment your CSS selectors
9. **Version control** - Git commit after each new scraper
10. **Test in production** - Apify console first, then schedule

## ğŸ“ Learning Path

1. âœ… Understand the architecture (you're here!)
2. ğŸ“– Read ontario.js or samgov.js for examples
3. ğŸ§ª Test an existing scraper locally
4. ğŸ“‹ Copy _template.js for your new source
5. ğŸ¨ Customize the template
6. ğŸ§ª Test your scraper
7. ğŸ”— Integrate into main.js
8. ğŸš€ Deploy to Apify
9. ğŸ“… Schedule regular runs
10. ğŸ‰ Celebrate!

## ğŸ“ Need Help?

Check:
1. README.md (this file)
2. _template.js (starter code)
3. ontario.js or samgov.js (working examples)
4. Apify logs (debugging)
5. Playwright docs (selectors)

---

**Remember:** This modular architecture means you can have 20+ sources without a messy codebase. Each scraper is independent and maintainable! ğŸ‰